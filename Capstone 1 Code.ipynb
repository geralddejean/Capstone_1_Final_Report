{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import t\n",
    "from scipy.io.arff import loadarff\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#open .arff file if it exists in directory or fetch it from the web\n",
    "arff_file = '5year.arff'\n",
    "\n",
    "if path.exists(arff_file) == False:\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00365/data.zip'\n",
    "    r = requests.get(url)\n",
    "    textfile = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    textfile.extract(arff_file)\n",
    "\n",
    "file = open(arff_file, 'r')\n",
    "filedata = file.read()\n",
    "filedata = filedata.replace('class {0,1}','Attr65 numeric')\n",
    "\n",
    "file = open(arff_file, 'w')\n",
    "file.write(filedata)\n",
    "file.close()\n",
    "\n",
    "#Convert .arff file to a dataframe\n",
    "data = loadarff(arff_file)\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "# Show relavant statistics\n",
    "allStats = df.describe(include='all')\n",
    "\n",
    "# Show relevant statistics with outliers removed\n",
    "df_NO = df.loc[:,'Attr1':'Attr64']\n",
    "df_NO = df_NO[(df_NO >= df_NO.mean()-2*df_NO.std()) &\n",
    "                        (df_NO <= df_NO.mean()+2*df_NO.std())]\n",
    "df_NO['Target'] = df['Attr65']\n",
    "allStats_NO = df_NO.describe(include='all')\n",
    "\n",
    "#Fill all missing values with the mean (df1)\n",
    "df1 = df_NO.fillna(df_NO.mean())\n",
    "\n",
    "#Remove rows with any Nan values (df2)\n",
    "df2 = df_NO.dropna().reset_index(drop=True)\n",
    "\n",
    "#Scaling of predictors\n",
    "df_scale = df1\n",
    "df3 = df_scale.drop('Target', axis=1)\n",
    "df3_scaler = StandardScaler()\n",
    "df3a = df3_scaler.fit_transform(df3.values)\n",
    "df3 = pd.DataFrame(df3a, columns=df3.columns)\n",
    "df3 = df3.join(df_scale.Target)\n",
    "\n",
    "# Choose new dataframe to be df1, df2, or df3 (scaled)\n",
    "df_NO = df1\n",
    "\n",
    "#Show correlation matrix to see if Attr37 is highly correlated with anything\n",
    "corrMat = df_NO.corr()\n",
    "corrMat['Target'].nsmallest(1), corrMat['Target'].nlargest(2)\n",
    "\n",
    "# Create a dataframe of attributes and their meanings\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data'\n",
    "url_get = requests.get(url)\n",
    "soup = BeautifulSoup(url_get.content, 'lxml')\n",
    "with open('attributes.txt', 'w', encoding='utf-8') as f_out:\n",
    "    f_out.write(soup.prettify())\n",
    "    f_out.close()\n",
    "\n",
    "with open(\"attributes.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = lines[367:494]\n",
    "    f.close()\n",
    "    \n",
    "finance_def_df = pd.DataFrame({'expression':lines})\n",
    "finance_def_df = finance_def_df[finance_def_df.index%2 == 0]\n",
    "finance_def_df = finance_def_df.replace('^\\s{6}X[0-9]+\\t','',regex=True)\\\n",
    ".replace('$\\n','',regex=True)\n",
    "finance_def_df.index = df.loc[:,'Attr1':'Attr64'].columns\n",
    "\n",
    "# Create DataFrame of strong correlations (negative and positive) based on correlation threshold\n",
    "strongCorrMatrix = corrMat.unstack().reset_index()\n",
    "strongCorrMatrix.rename(columns={'level_0':'Pair1',\n",
    "                                 'level_1':'Pair2',0:'Correlation'}, inplace=True)\n",
    "corrThresh = 0.95\n",
    "strongCorrMatrix = strongCorrMatrix[((strongCorrMatrix['Correlation'] >= corrThresh) |\n",
    "        (strongCorrMatrix['Correlation'] <= -corrThresh)) &\n",
    "        (strongCorrMatrix['Pair1'] != strongCorrMatrix['Pair2'])]\n",
    "strongCorrMatrix.reset_index(drop=True, inplace=True)\n",
    "strongCorrMatrix = strongCorrMatrix.drop_duplicates(subset='Correlation', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate Attr7, Attr14, and Attr18\n",
    "same_column_check = df1[['Attr7','Attr14', 'Attr18']]\n",
    "attr7_14_slope, attr7_14_incept = np.polyfit(df_NO.Attr7, df_NO.Attr14, 1)\n",
    "attr7_18_slope, attr7_18_incept = np.polyfit(df_NO.Attr7, df_NO.Attr18, 1)\n",
    "attr14_18_slope, attr14_18_incept = np.polyfit(df_NO.Attr14, df_NO.Attr18, 1)\n",
    "\n",
    "plt.plot(df_NO.Attr7, df_NO.Attr14, marker='.', linestyle='none', color='blue', label='true data')\n",
    "plt.plot(df_NO.Attr7, attr7_14_incept+df_NO.Attr7*attr7_14_slope, color='green', label='regression line')\n",
    "plt.legend()\n",
    "plt.xlabel('Attr7')\n",
    "plt.ylabel('Attr14')\n",
    "plt.title('Regression and True Data of Attr7 vs Attr14')\n",
    "plt.savefig('Attr7_vs_Attr14.jpg')\n",
    "\n",
    "plt.plot(df_NO.Attr7, df_NO.Attr18, marker='.', linestyle='none', color='blue', label='true data')\n",
    "plt.plot(df_NO.Attr7, attr7_18_incept+df_NO.Attr7*attr7_18_slope, color='green', label='regression line')\n",
    "plt.legend()\n",
    "plt.xlabel('Attr7')\n",
    "plt.ylabel('Attr18')\n",
    "plt.title('Regression and True Data of Attr7 vs Attr18')\n",
    "plt.savefig('Attr7_vs_Attr18.jpg')\n",
    "\n",
    "plt.plot(df_NO.Attr14, df_NO.Attr18, marker='.', linestyle='none', color='blue', label='true data')\n",
    "plt.plot(df_NO.Attr14, attr14_18_incept+df_NO.Attr14*attr14_18_slope, color='green', label='regression line')\n",
    "plt.legend()\n",
    "plt.xlabel('Attr14')\n",
    "plt.ylabel('Attr18')\n",
    "plt.title('Regression and True Data of Attr14 vs Attr18')\n",
    "plt.savefig('Attr14_vs_Attr18.jpg')\n",
    "\n",
    "same_column = pd.DataFrame({'Definition':[finance_def_df.loc[i,'expression']\\\n",
    "        for i in same_column_check.columns[0:3]]}, index=same_column_check.columns[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histogram of correlations for Target\n",
    "sns.distplot(corrMat['Target'][corrMat['Target'] != 1],bins=10,kde=False,norm_hist=False, color='red')\n",
    "plt.ylim(0,25)\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Frequency of Occurrence')\n",
    "plt.title('2012 Correlations of Attributes')\n",
    "plt.savefig('Correlations_Target_2012.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 most corelated attributes with Target\n",
    "top_ten = corrMat[['Target']][(corrMat['Target']) != 1]\n",
    "top_ten['mag'] = abs(top_ten.Target)\n",
    "top_ten = top_ten.nlargest(10, columns=['mag'])\n",
    "top_ten['neg'] = np.where(top_ten.Target < 0, top_ten.mag, 0)\n",
    "top_ten['pos'] = np.where(top_ten.Target > 0, top_ten.mag, 0)\n",
    "top_ten = top_ten.sort_values('mag', ascending=True)\n",
    "\n",
    "plt.barh(top_ten.index, top_ten.neg, color='red', label='negative correlation')\n",
    "plt.barh(top_ten.index, top_ten.pos, color='blue', label='positive correlation')\n",
    "plt.legend()\n",
    "plt.grid(axis='x')\n",
    "plt.xlim(0,0.35)\n",
    "plt.xlabel('Correlation Magnitude')\n",
    "plt.ylabel('Attribute')\n",
    "plt.title('Correlation Magnitudes by Attributes')\n",
    "plt.savefig('Correlations_magnitudes_Bar_2012.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe of Top 10 most corelated attributes with Target\n",
    "top_ten_att = pd.DataFrame({'Definition':[finance_def_df.loc[i,'expression']\\\n",
    "                                          for i in top_ten.index]}, index=top_ten.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average bankrupt companies over past 5 years\n",
    "bankrupt_companies = np.array([271,400,495,515,410])\n",
    "total_companies = np.array([7027,10173,10503,9792,5910])\n",
    "bankrupt_percentage = bankrupt_companies/total_companies*100\n",
    "sns.barplot([2008,2009,2010,2011,2012],bankrupt_percentage, color='red')\n",
    "plt.grid(axis='y')\n",
    "plt.ylim(0,8)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percent (%)')\n",
    "plt.title('Polish Bankruptcy Percentage by Year')\n",
    "plt.savefig('Bankruptcy_percentage_year.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms of data\n",
    "plt.subplots(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "for index in range(0,16):\n",
    "    plt.subplot(4,4,index+1)\n",
    "    df_NO.iloc[:,index].hist(bins=100)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Occurences')\n",
    "    plt.ylim(top=df_NO.iloc[:,index].value_counts(bins=100).max())\n",
    "    plt.legend((df_NO.columns[index],), loc=0)\n",
    "\n",
    "plt.subplots(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "for index in range(16,32):\n",
    "    plt.subplot(4,4,index-15)\n",
    "    df_NO.iloc[:,index].hist(bins=100)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Occurences')\n",
    "    plt.ylim(top=df_NO.iloc[:,index].value_counts(bins=100).max())\n",
    "    plt.legend((df_NO.columns[index],), loc=0)\n",
    "\n",
    "plt.subplots(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "for index in range(32,48):\n",
    "    plt.subplot(4,4,index-31)\n",
    "    df_NO.iloc[:,index].hist(bins=100)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Occurences')\n",
    "    plt.ylim(top=df_NO.iloc[:,index].value_counts(bins=100).max())\n",
    "    plt.legend((df_NO.columns[index],), loc=0)\n",
    "\n",
    "plt.subplots(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.4)\n",
    "for index in range(48,64):\n",
    "    plt.subplot(4,4,index-47)\n",
    "    df_NO.iloc[:,index].hist(bins=100)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Occurences')\n",
    "    plt.ylim(top=df_NO.iloc[:,index].value_counts(bins=100).max())\n",
    "    plt.legend((df_NO.columns[index],), loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis Testing\n",
    "\n",
    "#1 - H0: mean(Attr2) + mean(Attr10) = 1 \n",
    "\n",
    "mean_attr2 = df_NO.Attr2.mean()\n",
    "mean_attr10 = df_NO.Attr10.mean()\n",
    "\n",
    "sum_mean = mean_attr2 + mean_attr10\n",
    "CI_mean_attr2 = t.interval(0.95, len(df_NO.Attr2)-1,\\\n",
    "                           loc=mean_attr2, scale=ss.sem(df_NO.Attr2))\n",
    "CI_mean_attr10 = t.interval(0.95, len(df_NO.Attr10)-1,\\\n",
    "                           loc=mean_attr2, scale=ss.sem(df_NO.Attr10))\n",
    "\n",
    "# Permutations are ONLY for unequal array samples\n",
    "#This is just one example where we are summing the mean\n",
    "def permutation_replicates(array1, array2, size):\n",
    "    \"\"\"Generate multiple permutation replicates.\"\"\"\n",
    "\n",
    "    array = np.concatenate((array1, array2))\n",
    "    \n",
    "    perm_sum_mean = np.empty(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        perm_array = np.random.permutation(array)\n",
    "        perm_array1 = perm_array[:len(array1)]\n",
    "        perm_array2 = perm_array[len(array1):]\n",
    "        perm_sum_mean[i] = np.mean(perm_array1) + np.mean(perm_array2)\n",
    "        \n",
    "    return perm_sum_mean\n",
    "\n",
    "sum_mean_perm = permutation_replicates(df_NO.Attr2, df_NO.Attr10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More Hypothesis Testing\n",
    "\n",
    "#2\n",
    "sum_means = np.mean(df_NO.Attr2) + np.mean(df_NO.Attr10)\n",
    "\n",
    "def bootstrap_replicates(array1, array2, size):\n",
    "    \"\"\"Build bootstrap replicates for two arrays\"\"\"\n",
    "    \n",
    "    bs_sum_mean = np.empty(size)\n",
    "        \n",
    "    for i in range(size):\n",
    "        bs_array1 = np.random.choice(array1)\n",
    "        bs_array2 = np.random.choice(array2)\n",
    "        bs_sum_mean[i] = np.mean(bs_array1) + np.mean(bs_array2)\n",
    "        \n",
    "    return bs_sum_mean\n",
    "\n",
    "bs_sum_means = bootstrap_replicates(df_NO.Attr2, df_NO.Attr10, 10000)\n",
    "plt.hist(bs_sum_means, bins=100)\n",
    "\n",
    "p_value = np.sum(bs_sum_means <= sum_means)/len(bs_sum_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More Hypothesis Testing\n",
    "\n",
    "#3\n",
    "attr39_0 = df_NO.Attr39[df_NO.Target == 0]\n",
    "attr39_1 = df_NO.Attr39[df_NO.Target == 1]\n",
    "\n",
    "_ = plt.hist(attr39_0, bins=30)\n",
    "_ = plt.axvline(x=np.mean(attr39_0), color='black', linestyle='dashed')\n",
    "_ = plt.text(-0.65,1760,'Mean = {:.3f}'.format(np.mean(attr39_0)))\n",
    "_ = plt.xlabel('Profit on Sales vs. Sales')\n",
    "_ = plt.ylabel('Counts')\n",
    "_ = plt.title('Distribution of Means of Profit on Sales vs. Sales\\nfor Non-bankrupt Companies')\n",
    "plt.show()\n",
    "\n",
    "_ = plt.hist(attr39_1, bins=30)\n",
    "_ = plt.axvline(x=np.mean(attr39_0), color='black', linestyle='dashed')\n",
    "_ = plt.text(-0.5,100,'Mean = {:.3f}'.format(np.mean(attr39_1)))\n",
    "_ = plt.xlabel('Profit on Sales vs. Sales')\n",
    "_ = plt.ylabel('Counts')\n",
    "_ = plt.title('Distribution of Means of Profit on Sales vs. Sales\\nfor Bankrupt Companies')\n",
    "plt.show()\n",
    "\n",
    "means = (np.mean(attr39_0), np.mean(attr39_1), np.mean(df_NO.Attr39)) \n",
    "diff_means = np.abs(means[0] - means[1])\n",
    "\n",
    "attr39_0_shifted = attr39_0 - means[0] + means[2]\n",
    "attr39_1_shifted = attr39_1 - means[1] + means[2]\n",
    "\n",
    "def bootstrap_replicates(array1, array2, size):\n",
    "    \"\"\"Build bootstrap replicates for two arrays\"\"\"\n",
    "    \n",
    "    bs_means_array1 = np.empty(size)\n",
    "    bs_means_array2 = np.empty(size)\n",
    "    \n",
    "    for i in range(size):\n",
    "        bs_array1 = np.random.choice(array1)\n",
    "        bs_array2 = np.random.choice(array2)\n",
    "        bs_means_array1[i], bs_means_array2[i] = np.mean(bs_array1), np.mean(bs_array2)\n",
    "        \n",
    "    return bs_means_array1, bs_means_array2\n",
    "\n",
    "bs_attr39_0, bs_attr39_1 = bootstrap_replicates(attr39_0_shifted, attr39_1_shifted, 10000)\n",
    "bs_diff_means = np.abs(bs_attr39_0 - bs_attr39_1)\n",
    "_ = plt.hist(bs_diff_means, bins=30, color='green')\n",
    "_ = plt.axvline(x=diff_means, color='black', linestyle='dashed')\n",
    "_ = plt.text(0.19,2000,'Experimental Mean = {:.3f}'.format(diff_means))\n",
    "_ = plt.xlabel('Difference of Means')\n",
    "_ = plt.ylabel('Counts')\n",
    "_ = plt.title('Difference of Means of Profit on Sales vs. Sales\\nfor Both Types of Companies')\n",
    "plt.savefig('Bootstrapped_Difference_of_Means.png')\n",
    "plt.show()\n",
    "\n",
    "p_value = np.sum(bs_diff_means >= diff_means)/len(bs_diff_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Setup Stage\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "X = df_NO.drop(['Target'], axis=1) #Dependent Variables\n",
    "y = df_NO['Target'].astype('int64')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Implement Variable Inflation factor\n",
    "\n",
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv)\n",
    "\n",
    "final_iv, IV = data_vars(X_train, y_train)\n",
    "\n",
    "features = list(IV[(IV['IV'] >= 0.01) & (IV['IV'] <= 0.8)]['VAR_NAME'])\n",
    "X2 = X_train[features]\n",
    "\n",
    "def iterate_vif(df, vif_threshold=5, max_vif=6):\n",
    "  count = 0\n",
    "  while max_vif > vif_threshold:\n",
    "    count += 1\n",
    "    print(\"Iteration # \"+str(count))\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIFactor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    vif[\"features\"] = df.columns\n",
    "    \n",
    "    if vif['VIFactor'].max() > vif_threshold:\n",
    "      print('Removing %s with VIF of %f' % (vif[vif['VIFactor'] == vif['VIFactor'].max()]['features'].values[0], vif['VIFactor'].max()))\n",
    "      df = df.drop(vif[vif['VIFactor'] == vif['VIFactor'].max()]['features'].values[0], axis=1)\n",
    "      max_vif = vif['VIFactor'].max()\n",
    "    else:\n",
    "        print('Complete')\n",
    "        return df, vif.sort_values('VIFactor')\n",
    "    \n",
    "X1 = X2._get_numeric_data()\n",
    "final_df, final_vif = iterate_vif(X1)\n",
    "\n",
    "X_train = final_df\n",
    "X_test=X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe of Compressed Feature Definitions with VIFs less than 5\n",
    "fin_def_vif = pd.DataFrame(index=X_train.columns)\n",
    "fin_def_vif = pd.merge(fin_def_vif, finance_def_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Optional) Creating model with reduced features that are statistically significant\n",
    "\n",
    "# df3-2: Compressed features by eliminating those that are not statistically significant\n",
    "X_train = X_train[['Attr22','Attr24','Attr25','Attr3','Attr48','Attr49','Attr55','Attr56','Attr62']]\n",
    "X_test=X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a machine Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Logistic Regression\n",
    "\n",
    "def run_regression_accuracy(X_train, y_train, X_test, y_test):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('\\nConfusion matrix: \\n',cm)\n",
    "\n",
    "    print('\\nClassification report: \\n',classification_report(y_test, y_pred))\n",
    "\n",
    "    logit_roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "#    plt.savefig('LogReg_ROC', dpi=300)\n",
    "    plt.show()\n",
    "  \n",
    "    return logreg, fpr, tpr, thresholds\n",
    "\n",
    "logreg, fpr, tpr, thresholds = run_regression_accuracy(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression Summary\n",
    "\n",
    "import statsmodels.api as sm\n",
    "sm_model = sm.Logit(y_train, sm.add_constant(X_train)).fit()\n",
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for XGBoost and LightGBM\n",
    "\n",
    "def GridSearchCV_Tuning(param_grid, classifier):\n",
    "    \n",
    "    model_class = classifier\n",
    "    model_class_cv = GridSearchCV(model_class, param_grid, cv=10)\n",
    "    \n",
    "    model_class_cv.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\nBest Parameters: \\n', model_class_cv.best_params_)\n",
    "    print('\\nBest Score: \\n', model_class_cv.best_score_)\n",
    "    \n",
    "# Different classifiers: xgb.XGBClassifier(), lgb.LGBMClassifier()\n",
    "\n",
    "param_grid = {'max_depth':np.arange(4,10), 'learning_rate':[0.0001, 0.001, 0.01, 0.1], \\\n",
    "              'n_estimators':[10, 20, 30, 40, 50]}\n",
    "\n",
    "GridSearchCV_Tuning(param_grid, lgb.LGBMClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 XGBoost\n",
    "\n",
    "def XGBoost_accuracy(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    #df1: learning_rate=0.1, max_depth=4, n_estimators=50, seed=42\n",
    "    #df2: learning_rate=0.1, max_depth=7, n_estimators=50, seed=42\n",
    "    #df3-1: learning_rate=0.1, max_depth=4, n_estimators=40, seed=42\n",
    "    #df3-2: learning_rate=0.1, max_depth=5, n_estimators=40, seed=42\n",
    "    xgbclass = xgb.XGBClassifier(learning_rate=0.1, max_depth=4 , objective='binary:logistic', \\\n",
    "                             n_estimators=40, seed=42)\n",
    "    xgbclass.fit(X_train, y_train)\n",
    "    y_pred = xgbclass.predict(X_test)\n",
    "    \n",
    "    accuracy = float(np.sum(y_pred==y_test))/y_test.shape[0]\n",
    "    print('\\nAccuracy is {:.2f}\\n'.format(accuracy))\n",
    "    print('\\nConfusion Matrix: \\n',confusion_matrix(y_test, y_pred))\n",
    "    print('\\nClassification Report: \\n',classification_report(y_test, y_pred))\n",
    "    \n",
    "    fpr1, tpr1, thresholds = roc_curve(y_test, xgbclass.predict_proba(X_test)[:,1])\n",
    "    auc_score1 = roc_auc_score(y_test, xgbclass.predict_proba(X_test)[:,1])\n",
    "    plt.plot(fpr1, tpr1, color='orange', label='XGBoost (area = %0.3f)' % auc_score1)\n",
    "    plt.plot([0, 1],[0, 1], color='red', ls='--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1.05])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "#    plt.savefig('XGBoost_ROC', dpi=300)\n",
    "    plt.show()\n",
    "        \n",
    "    return xgbclass, fpr1, tpr1\n",
    "\n",
    "xgbclass, fpr1, tpr1 = XGBoost_accuracy(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 LightGBM\n",
    "\n",
    "def LightGBM_accuracy(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    #df1: learning_rate=0.1, max_depth=4, n_estimators=50, seed=42\n",
    "    #df2: learning_rate=0.1, max_depth=9, n_estimators=40, seed=42\n",
    "    #df3-1: learning_rate=0.1, max_depth=5, n_estimators=40, seed=42\n",
    "    #df3-2: learning_rate=0.1, max_depth=6, n_estimators=30, seed=42\n",
    "    lgbmclass = lgb.LGBMClassifier(learning_rate=0.1, max_depth=6, objective='binary', \\\n",
    "                             n_estimators=30, seed=42)\n",
    "    lgbmclass.fit(X_train, y_train)\n",
    "    y_pred = lgbmclass.predict(X_test)\n",
    "    \n",
    "    accuracy = float(np.sum(y_pred==y_test))/y_test.shape[0]\n",
    "    print('\\nAccuracy is {:.2f}\\n'.format(accuracy))\n",
    "    print('\\nConfusion Matrix: \\n',confusion_matrix(y_test, y_pred))\n",
    "    print('\\nClassification Report: \\n',classification_report(y_test, y_pred))\n",
    "    \n",
    "    fpr2, tpr2, thresholds = roc_curve(y_test, lgbmclass.predict_proba(X_test)[:,1])\n",
    "    auc_score2 = roc_auc_score(y_test, lgbmclass.predict_proba(X_test)[:,1])\n",
    "    plt.plot(fpr2, tpr2, color='green', label='LightGBM (area = %0.3f)' % auc_score2)\n",
    "    plt.plot([0, 1],[0, 1], color='red', ls='--')\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1.05])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "#    plt.savefig('LightGBM_ROC', dpi=300)\n",
    "    plt.show()\n",
    "        \n",
    "    return lgbmclass, fpr2, tpr2\n",
    "\n",
    "lgbmclass, fpr2, tpr2 = LightGBM_accuracy(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
